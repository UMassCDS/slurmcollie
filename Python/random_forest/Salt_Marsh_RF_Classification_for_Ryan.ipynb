{"cells":[{"cell_type":"markdown","metadata":{"id":"PmknlbLyPF2J"},"source":["\n","### This version does not use dask to load imagery\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M2Gz90bpcNj3"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","#comment test, 15 Sep 2023"]},{"cell_type":"markdown","source":["#Steve comment\n","\n","I have not tried to untangle most of the code below, just focusing on the random forest part. That said, my approach with what seems like a similar project is to separate out labeled data from unlabeled data. Then just load and use labeled data to do model training and testing.\n","\n","It appears you have roughly 105M samples/rows of total pixels, each at 140 bands. Huge. My guess you have actually only labeled a fraction of that, say 100K. It seems you should only be working with that 100K in this notebook. Maybe that is all you are doing and I have not deciphered the code below well enough. But my take is that 100K samples with 140 features should likely run directly in Colab, probably with the Pro+ version. I'd be happy to try it :)\n","\n","Another comment I leave below is that I only preprocess my data once and then store the resulting numpy matrix out to file. I then can just load this file subsequently, skipping all the preprocessing steps. My numpy matrix is going to be about the same size as yours and I can tell you it is relatively fast to load it. Again, maybe you are doing this and I haven't untangled the code to see it.\n","\n","I did not see any testing going on. It appears you rely totally on oob scoring and the training set for evaluation. If so, I believe you might want to rethink this. Standard practice is to show results from a test set. Much more reliable for what will happen in wild.\n","\n","Finally, I urge you to consider a more modern modeling approach based on boosting. It is kind of the next generation of tree ensembles. It should not cause you to change much and could have many benefits."],"metadata":{"id":"qhyeEUJ3gvp2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hlWn28OfcTGq"},"outputs":[],"source":["from osgeo import gdal, ogr, gdal_array # I/O image data\n","import numpy as np # math and array handling\n","import matplotlib.pyplot as plt # plot figures\n","from sklearn.ensemble import RandomForestClassifier # classifier\n","import pandas as pd # handling large data as table sheets\n","from sklearn.metrics import classification_report, accuracy_score,confusion_matrix  # calculating measures for accuracy assessment\n","\n","import seaborn as sn\n","\n","import datetime\n","\n","import joblib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uPX1k_NX5cc-"},"outputs":[],"source":["import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oc4kvzp0w4e1"},"outputs":[],"source":["import os\n","\n","# print(os.environ['GDAL_NUM_THREADS'])\n","os.environ['PROJ_LIB'] = \"/work/pi_gstuart_umass_edu/kate/conda/share/proj\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bneZO374zh9x","outputId":"bcc96f9d-8cd3-4252-c0ca-eedd552a5ad1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Available attributes in the shape file are: ['Transect', 'PointNum', 'SubClass', 'Northing', 'Easting', 'Altitude', 'Notes', 'Class', 'Raw Subcla', 'Pre Angle', 'Post Angle', 'mu', 'Class mu', 'Pre/Post', 'Hydro', 'ReClaSch A']\n"]}],"source":["# Tell GDAL to throw Python exceptions, and register all drivers\n","gdal.UseExceptions()\n","gdal.AllRegister()\n","\n","# define a number of trees that should be used (default = 500)\n","est = 500\n","\n","# how many cores should be used?\n","# -1 -> all available cores\n","n_cores = 60\n","\n","# the remote sensing image you want to classify\n","img_RS = r'work/pi_cschweik_umass_edu/Ryan/Modeling_stack/Mid_High_Stacked_Clipped.tif'\n","\n","# training and validation as shape files\n","training = r'/work/pi_gstuart_umass_edu/kate/OTH_Training_Data/OTH_Polygons_v3_relassed.shp'\n","validation = r'/work/pi_gstuart_umass_edu/kate/OTH_Training_Data/OTH_Polygons_v3_relassed.shp'\n","\n","# what is the attributes name of your classes in the shape file (field name of the classes)?\n","attribute = 'ReClaSch A'\n","\n","\n","# directory, where the classification image should be saved:\n","classification_image = r'/work/pi_gstuart_umass_edu/kate/Classification_products/OTH_all_reclass_classification_v1.gtif'\n","\n","# directory, where the all meta results should be saved:\n","results_txt = r'/work/pi_gstuart_umass_edu/kate/Classification_products/OTH_all_reclass_classification_v1.txt'\n","\n","# laod training data and show all shape attributes\n","\n","#model_dataset = gdal.Open(model_raster_fname)\n","shape_dataset = ogr.Open(training)\n","shape_layer = shape_dataset.GetLayer()\n","\n","# extract the names of all attributes (fieldnames) in the shape file\n","attributes = []\n","ldefn = shape_layer.GetLayerDefn()\n","for n in range(ldefn.GetFieldCount()):\n","    fdefn = ldefn.GetFieldDefn(n)\n","    attributes.append(fdefn.name)\n","\n","# print the attributes\n","print('Available attributes in the shape file are: {}'.format(attributes))\n","\n","# prepare results text file:\n","\n","print('Random Forest Classification', file=open(results_txt, \"a\"))\n","print('Processing: {}'.format(datetime.datetime.now()), file=open(results_txt, \"a\"))\n","print('-------------------------------------------------', file=open(results_txt, \"a\"))\n","print('PATHS:', file=open(results_txt, \"a\"))\n","print('Image: {}'.format(img_RS), file=open(results_txt, \"a\"))\n","print('Training shape: {}'.format(training) , file=open(results_txt, \"a\"))\n","print('Vaildation shape: {}'.format(validation) , file=open(results_txt, \"a\"))\n","print('      choosen attribute: {}'.format(attribute) , file=open(results_txt, \"a\"))\n","print('Classification image: {}'.format(classification_image) , file=open(results_txt, \"a\"))\n","print('Report text file: {}'.format(results_txt) , file=open(results_txt, \"a\"))\n","print('-------------------------------------------------', file=open(results_txt, \"a\"))"]},{"cell_type":"markdown","source":["#Steve comment\n","\n","If below takes a long time, is there no way to cache it? Compute it once but then store results (looks like numpy array) out to file. Then just load that file in in place of this code? I do that with my project and gives me huge time savings after you bite the bullet once."],"metadata":{"id":"A4hX321X5r9Y"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gqKOJGiGw4e3","outputId":"c5dd18ec-5708-4b8d-d014-52cc2c86f332"},"outputs":[{"name":"stdout","output_type":"stream","text":["(12434, 13246, 142) <class 'numpy.float32'> 4\n","4\n","0\n","1\n"]}],"source":["# load image data\n","######THIS IS STEP THAT TAKES A LONG TIME#######\n","\n","img_ds = gdal.Open(img_RS, gdal.GA_ReadOnly)\n","\n","img = np.zeros((img_ds.RasterYSize, img_ds.RasterXSize, img_ds.RasterCount),\n","               gdal_array.GDALTypeCodeToNumericTypeCode(img_ds.GetRasterBand(1).DataType))\n","print(img.shape, gdal_array.GDALTypeCodeToNumericTypeCode(img_ds.GetRasterBand(1).DataType), img.itemsize)\n","print(img_ds.GetRasterBand(1).ReadAsArray().itemsize)\n","for b in range(img.shape[2]):\n","    print(b)\n","    img[:, :, b] = img_ds.GetRasterBand(b + 1).ReadAsArray()\n","\n","row = img_ds.RasterYSize\n","col = img_ds.RasterXSize\n","band_number = img_ds.RasterCount\n","\n","print('Image extent: {} x {} (row x col)'.format(row, col))\n","print('Number of Bands: {}'.format(band_number))\n","\n","\n","print('Image extent: {} x {} (row x col)'.format(row, col), file=open(results_txt, \"a\"))\n","print('Number of Bands: {}'.format(band_number), file=open(results_txt, \"a\"))\n","print('---------------------------------------', file=open(results_txt, \"a\"))\n","print('TRAINING', file=open(results_txt, \"a\"))\n","print('Number of Trees: {}'.format(est), file=open(results_txt, \"a\"))"]},{"cell_type":"markdown","source":["#Steve comment\n","\n","It appears to me we will end up with 12kx13k samples/rows (156M) with 142 bands each. That is huge and will impact memory significantly. News you already know :)\n","\n","However, I assume only a small fraction is actually labeled and suitable for training. I am going to guess 100K samples have been labeled.\n","\n","I am going to suggest a couple things down below to (a) deal with memory and (b) improve the model."],"metadata":{"id":"7oQ9l1S25BZ_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2QGw1H4w4e4"},"outputs":[],"source":["# laod training data from shape file\n","# img_ds = gdal.Open(img_RS, gdal.GA_ReadOnly)\n","\n","#model_dataset = gdal.Open(model_raster_fname)\n","shape_dataset = ogr.Open(training)\n","shape_layer = shape_dataset.GetLayer()\n","\n","mem_drv = gdal.GetDriverByName('MEM')\n","mem_raster = mem_drv.Create('',img_ds.RasterXSize,img_ds.RasterYSize,1,gdal.GDT_UInt16)\n","#mem_raster.SetProjection(img_ds.GetProjection())\n","mem_raster.SetGeoTransform(img_ds.GetGeoTransform())\n","mem_band = mem_raster.GetRasterBand(1)\n","mem_band.Fill(0)\n","mem_band.SetNoDataValue(0)\n","\n","att_ = 'ATTRIBUTE='+attribute\n","# http://gdal.org/gdal__alg_8h.html#adfe5e5d287d6c184aab03acbfa567cb1\n","# http://gis.stackexchange.com/questions/31568/gdal-rasterizelayer-doesnt-burn-all-polygons-to-raster\n","err = gdal.RasterizeLayer(mem_raster, [1], shape_layer, None, None, [1],  [att_,\"ALL_TOUCHED=TRUE\"])\n","assert err == gdal.CE_None\n","\n","roi = mem_raster.ReadAsArray()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JavpcBydw4e4"},"outputs":[],"source":["# # Display images\n","# plt.subplot(121)\n","# plt.imshow(img[:, :, 0], cmap=plt.cm.Greys_r)\n","# plt.title('RS image - first band')\n","\n","# plt.subplot(122)\n","# plt.imshow(roi, cmap=plt.cm.Spectral)\n","# plt.title('Training Image')\n","\n","# plt.show()\n","\n","# Number of training pixels:\n","n_samples = (roi > 0).sum()\n","print('{n} training samples'.format(n=n_samples))\n","print('{n} training samples'.format(n=n_samples), file=open(results_txt, \"a\"))\n","\n","# What are our classification labels?\n","labels = np.unique(roi[roi > 0])\n","print('training data include {n} classes: {classes}'.format(n=labels.size, classes=labels))\n","print('training data include {n} classes: {classes}'.format(n=labels.size, classes=labels), file=open(results_txt, \"a\"))\n","\n","# Subset the image dataset with the training image = X\n","# Mask the classes on the training dataset = y\n","# These will have n_samples rows\n","X = img[roi > 0, :]\n","y = roi[roi > 0]\n","\n","print('Our X matrix is sized: {sz}'.format(sz=X.shape))\n","print('Our y array is sized: {sz}'.format(sz=y.shape))"]},{"cell_type":"markdown","source":["#Steve comment\n","\n","I'm wondering if your classifier a bit underspecified. And wonder if we can make it more efficient (less of a time and memory hog). Here is a proposed change:\n","<pre>\n","rf = RandomForestClassifier(\n","    n_estimators=100,      # Start lower, let warm_start add more\n","    max_depth=None,        # Can let trees grow fully\n","    min_samples_leaf=1,\n","    max_features='sqrt',   # sqrt(140) â‰ˆ 12 features per split\n","    oob_score=True,\n","    warm_start=True,       # finding optimal n_estimators\n","    n_jobs=-1,\n","    verbose=1,\n","    random_state=42        # for reproducibility\n",")\n","</pre>\n","\n","###train-test split\n","\n","I did not see it but assume at some point you will split your labeled data into a training set and a test set.\n","<pre>\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y,\n","    test_size=0.2,\n","    stratify=y,\n","    random_state=42  #for reproducibility\n",")\n","\n","#Training set: 80K rows.\n","#Test set: 20K rows.\n","\n","# Train incrementally - potential memory savings\n","step = 100  # Add 100 trees each time\n","max_total_estimators = 1000\n","plateaued = 0\n","best_n_estimators = None\n","\n","# First fit\n","rf.fit(X_train, y_train)\n","current_estimators = rf.n_estimators_\n","min_oob_score = rf.oob_score_\n","\n","while current_estimators < max_total_estimators:\n","    # Add 100 more trees\n","    current_estimators += step\n","    rf.set_params(n_estimators=current_estimators)\n","    rf.fit(X_train, y_train)  #adds whatever the difference is between the current n_estimators and what's already there.\n","    \n","    # Check if OOB score improved\n","    if rf.oob_score_ > min_oob_score + 0.0001:\n","        min_oob_score = rf.oob_score_\n","        best_n_estimators = rf.n_estimators_  # Track the best point\n","        plateaued = 0\n","    else:\n","        plateaued += 1\n","    \n","    if plateaued == 3:  # No improvement for 3 iterations\n","        break\n","\n","# Roll back to best number of estimators\n","if best_n_estimators:\n","    rf.set_params(n_estimators=best_n_estimators)\n","    rf.fit(X_train, y_train)  # One final fit to get back to best state\n","\n","# For memory efficiency, evaluate in chunks:\n","\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\n","\n","def evaluate_in_chunks(model, X, y, chunk_size=1000000):\n","    predictions = []\n","    n_samples = len(X)\n","    \n","    # Get predictions in chunks\n","    for i in range(0, n_samples, chunk_size):\n","        chunk_end = min(i + chunk_size, n_samples)\n","        X_chunk = X[i:chunk_end]\n","        chunk_preds = model.predict(X_chunk)\n","        predictions.extend(chunk_preds)\n","    \n","    predictions = np.array(predictions)\n","    \n","    # Calculate all metrics\n","    precision, recall, f1, _ = precision_recall_fscore_support(y, predictions, average='weighted')\n","    \n","    metrics = {\n","        'accuracy': accuracy_score(y, predictions),\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1,\n","        'confusion_matrix': confusion_matrix(y, predictions)\n","    }\n","    \n","    return metrics\n","\n","results = evaluate_in_chunks(rf, X_test, y_test)\n","\n","print(\"Accuracy:\", results['accuracy'])\n","print(\"Precision:\", results['precision'])\n","print(\"Recall:\", results['recall'])\n","print(\"F1 Score:\", results['f1'])\n","print(\"\\nConfusion Matrix:\\n\", results['confusion_matrix'])\n","</pre>"],"metadata":{"id":"QpcyeCtS0oFW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZrzBnJv0w4e5"},"outputs":[],"source":["rf = RandomForestClassifier(n_estimators=est, oob_score=True, verbose=1, n_jobs=n_cores)\n","\n","# verbose = 2 -> prints out every tree progression\n","# rf = RandomForestClassifier(n_estimators=est, oob_score=True, verbose=2, n_jobs=n_cores)\n","\n","\n","\n","X = np.nan_to_num(X)\n","rf2 = rf.fit(X, y)\n","\n","\n","# Save the trained model to a file\n","pickle_file = '/work/pi_gstuart_umass_edu/kate/Classification_products/RF_alldata_v1.pkl'\n","with open(pickle_file, 'wb') as file:\n","    pickle.dump(rf2, file)\n","\n","print ('model pickled')\n","\n","# With our Random Forest model fit, we can check out the \"Out-of-Bag\" (OOB) prediction score:\n","\n","print('--------------------------------', file=open(results_txt, \"a\"))\n","print('TRAINING and RF Model Diagnostics:', file=open(results_txt, \"a\"))\n","print('OOB prediction of accuracy is: {oob}%'.format(oob=rf.oob_score_ * 100))\n","print('OOB prediction of accuracy is: {oob}%'.format(oob=rf.oob_score_ * 100), file=open(results_txt, \"a\"))\n","\n","\n","# we can show the band importance:\n","bands = range(1,img_ds.RasterCount+1)\n","\n","for b, imp in zip(bands, rf2.feature_importances_):\n","    print('Band {b} importance: {imp}'.format(b=b, imp=imp))\n","    print('Band {b} importance: {imp}'.format(b=b, imp=imp), file=open(results_txt, \"a\"))\n","\n","\n","# Let's look at a crosstabulation to see the class confusion.\n","# To do so, we will import the Pandas library for some help:\n","# Setup a dataframe -- just like R\n","# Exception Handling because of possible Memory Error\n","\n","try:\n","    df = pd.DataFrame()\n","    df['truth'] = y\n","    df['predict'] = rf.predict(X)\n","\n","except MemoryError:\n","    print('Crosstab not available ')\n","\n","else:\n","    # Cross-tabulate predictions\n","    print(pd.crosstab(df['truth'], df['predict'], margins=True))\n","    print(pd.crosstab(df['truth'], df['predict'], margins=True), file=open(results_txt, \"a\"))\n","\n","# Predicting the rest of the image\n","\n","# generate mask image from red band\n","mask = np.copy(img[:,:,0])\n","mask[mask > 0.0] = 1.0 # all actual pixels have a value of 1.0\n","\n","# Take our full image and reshape into long 2d array (nrow * ncol, nband) for classification\n","old_shape = img.shape\n","new_shape = (img.shape[0] * img.shape[1], img.shape[2])\n","# img = img[:, :, :int(img.shape[2])].reshape(new_shape)\n","img = img.reshape(new_shape)\n","\n","print('Reshaped from {o} to {n}'.format(o=old_shape, n=img.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrB7XKyDr3LH"},"outputs":[],"source":["# img = np.nan_to_num(img)\n","img[np.isnan(img)] = 0.0\n","# class_prediction = rf.predict(img)\n","\n","print (\"DONE!\")\n"]},{"cell_type":"markdown","source":["#Steve comment\n","\n","My suggestion is to go with boosting over random forest. It is more memory efficient and typically gives better results (however see my caveats below). It builds on the RF ensemble idea but has true learning. I suggested this to Kate earlier but she ran into errors with it. I could see that given early versions were flakey. But now stable.  Here would be my top choice:\n","\n","<pre>\n","from lightgbm import LGBMClassifier\n","from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import randint, uniform\n","</pre>\n","\n","###If going to forego tuning, here is my best guess\n","\n","<pre>\n","num_classes = 4  #not sure about this\n","\n","final_model = LGBMClassifier(\n","    n_estimators=1000,         # Use early stopping to minimize\n","    learning_rate=0.1,        # Standard value, reliable\n","    num_leaves=31,           # Default, good balance of complexity/performance\n","    max_depth=6,             # Prevent overfitting\n","    min_child_samples=20,    # ~0.02% of data\n","    subsample=0.8,           # Use 80% of data per tree\n","    colsample_bytree=0.8,    # Use 80% of features per tree\n","    reg_alpha=0.1,           # Light L1 regularization\n","    reg_lambda=0.1,          # Light L2 regularization\n","    n_jobs=-1,               # Use all cores for single model\n","    random_state=42,\n","    objective='multiclass',  # Specifically for n-class problem\n","    num_class=num_classes    # Specify number of classes\n",")\n","\n","final_model.fit(\n","    X_train, y_train,\n","    eval_set=[(X_val, y_val)],   #note will have to do two splitting rounds to get this\n","    early_stopping_rounds=20,\n","    eval_metric='multi_logloss'  # for multiclass\n",")\n","</pre>\n","\n","###Below I am introducing tuning, which I did not see in your code\n","\n","This is not optimized for memory efficiency. If memory becomes an issue, we can change parameter values to reduce memory usage.\n","\n","<pre>\n","param_distributions = {\n","    'learning_rate': uniform(0.01, 0.3),\n","    'num_leaves': randint(20, 255),\n","    'max_depth': randint(3, 30),\n","    'min_child_samples': randint(5, 50),\n","    'subsample': uniform(0.7, 0.3),\n","    'colsample_bytree': uniform(0.7, 0.3),\n","    'reg_alpha': uniform(0, 2),\n","    'reg_lambda': uniform(0, 2),\n","}\n","\n","# Can use RandomizedSearchCV\n","search = RandomizedSearchCV(\n","    LGBMClassifier(\n","        n_estimators=1000,  # Set high for early stopping\n","        n_jobs=-1\n","    ),\n","    param_distributions,\n","    n_iter=25,   #to save memory at cost of accuracy\n","    cv=[(slice(None), slice(None))],  # No CV splitting given using validation set\n","    scoring='accuracy',\n","    verbose=2,\n","    random_state=42,\n","    n_jobs=1  #to save memory at cost of time\n",")\n","\n","# Define fit parameters for early stopping\n","fit_params = {\n","    'eval_metric': 'multi_logloss',\n","    'early_stopping_rounds': 20,\n","    'eval_set': [(X_val, y_val)],   #requires two stage split\n","    'verbose': False\n","}\n","\n","search.fit(X_train, y_train, **fit_params)\n","\n","# Use best results\n","final_model = search.best_estimator_\n","print(\"Best parameters:\", search.best_params_)\n","print(\"Best score:\", search.best_score_)\n","print(\"Optimal number of trees:\", best_model.best_iteration_)\n","</pre>\n","\n","###Testing can still be done in chunks:\n","<pre>\n","results = evaluate_in_chunks(final_model, X_test, y_test)\n","\n","print(\"Accuracy:\", results['accuracy'])\n","print(\"Precision:\", results['precision'])\n","print(\"Recall:\", results['recall'])\n","print(\"F1 Score:\", results['f1'])\n","print(\"\\nConfusion Matrix:\\n\", results['confusion_matrix'])\n","</pre>\n","\n","##Reasons not to use boosting\n","\n","1. Training Speed:\n","- LightGBM is generally faster than XGBoost but slower than RF for a few reasons:\n","  - It needs to calculate gradients and update residuals sequentially\n","  - Each tree depends on previous predictions\n","  - However, its leaf-wise growth strategy is faster than level-wise tree growth\n","\n","2. Memory Usage with OOB:\n","- RF with OOB requires keeping all unused samples for each tree in memory\n","- LightGBM is typically more memory efficient because:\n","  - It uses histogram-based splitting\n","  - Doesn't need to store out-of-bag samples for each tree\n","  - Uses discretized feature values rather than raw values\n","\n","You can generally expect:\n","- Slower training than RF (though faster than XGBoost)\n","- Lower memory usage, especially with OOB enabled on RF\n","\n","However:\n","- The exact speed difference depends on your dataset size and feature count\n","- Memory usage advantage becomes more pronounced with larger datasets\n","- If you're using very small datasets, these differences might be negligible\n","\n","###More generally\n","\n","Random Forests work well with high variability because:\n","- Each tree gets a random subset of data and features\n","- Trees can grow deep to capture complex patterns\n","- Trees are independent, so they can model different aspects of the variability\n","- Averaging many diverse trees helps handle noise and outliers\n","\n","Boosting (like XGBoost/LightGBM) works better with more consistent patterns because:\n","- It builds trees sequentially, each focusing on correcting previous errors\n","- High variability can lead to overfitting as later trees may try too hard to fit noise\n","- The learning rate and sequential nature assume some underlying pattern to gradually improve upon\n","- Works best when there's a clear signal to learn from\n","\n","However:\n","- Both can work well on either type of data with proper tuning\n","- The distinction becomes more important with limited data\n","- Modern boosting implementations have features to help handle variability better"],"metadata":{"id":"a2rtzVXUECR-"}},{"cell_type":"markdown","metadata":{"id":"Hp8gwSplB0LJ"},"source":["# **Apply Prediction**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z4Sry-DPcVpQ"},"outputs":[],"source":["\n","slices = int(round(len(img)/20))\n","\n","test = True\n","\n","while test == True:\n","    try:\n","        class_preds = list()\n","\n","        temp = rf.predict(img[0:slices+1,:])\n","        class_preds.append(temp)\n","\n","        for i in range(slices,len(img),slices):\n","            print('{} %, derzeit: {}'.format((i*100)/(len(img)), i))\n","            temp = rf.predict(img[i+1:i+(slices+1),:])\n","            class_preds.append(temp)\n","\n","    except MemoryError as error:\n","        slices = slices/4\n","        print('Not enought RAM, new slices = {}'.format(slices))\n","\n","    else:\n","        test = False\n","else:\n","    print('Class prediction was successful without slicing!')\n","#concatenate all slices and re-shape it to the original extend\n","try:\n","    class_prediction = np.concatenate(class_preds,axis = 0)\n","except NameError:\n","    print('No slicing was necessary!')\n","\n","class_prediction = class_prediction.reshape(old_shape[:2])\n","print('Reshaped back to {}'.format(class_prediction.shape))\n","\n","\n","# # generate mask image from red band\n","# mask = np.copy(img[:,:,0])\n","# mask[mask > 0.0] = 1.0 # all actual pixels have a value of 1.0\n","\n","# plot mask\n","\n","# plt.imshow(mask)\n","\n","# mask classification an plot\n","\n","class_prediction.astype(np.float16)\n","class_prediction_ = class_prediction*mask\n","\n","cols = class_prediction.shape[1]\n","rows = class_prediction.shape[0]\n","\n","class_prediction_.astype(np.float16)\n","\n","driver = gdal.GetDriverByName(\"gtiff\")\n","outdata = driver.Create(classification_image, cols, rows, 1, gdal.GDT_UInt16)\n","outdata.SetGeoTransform(img_ds.GetGeoTransform())##sets same geotransform as input\n","outdata.SetProjection(img_ds.GetProjection())##sets same projection as input\n","outdata.GetRasterBand(1).WriteArray(class_prediction_)\n","outdata.FlushCache() ##saves to disk!!\n","del outdata\n","print('Image saved to: {}'.format(classification_image))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sujo4sNhw4e7"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuClass":"premium"},"gpuClass":"premium","kernelspec":{"display_name":"Salt Marsh","language":"python","name":"salt_marsh"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}